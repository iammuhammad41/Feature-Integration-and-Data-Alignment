{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "id": "W0_cz3El7rF_"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Load Data\n",
        "train = pd.read_csv('../input/birdsong-recognition/train.csv')\n",
        "test = pd.read_csv('../input/birdsong-recognition/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "T-_77Sx97rGB"
      },
      "cell_type": "code",
      "source": [
        "train.info(),test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bIFeImbJ7rGC"
      },
      "cell_type": "code",
      "source": [
        "total = train.isnull().sum().sort_values(ascending=False)\n",
        "percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "f, ax = plt.subplots(figsize=(15, 6))\n",
        "plt.xticks(rotation='90')\n",
        "sns.barplot(x=missing_data.index, y=missing_data['Percent'])\n",
        "plt.xlabel('Features', fontsize=15)\n",
        "plt.ylabel('Percent of Missing Values', fontsize=15)\n",
        "plt.title('Percentage of Missing Data by Feature', fontsize=15)\n",
        "missing_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "21jVPLXe7rGD"
      },
      "cell_type": "code",
      "source": [
        "# let's go ahead and have a look at how many observations we would drop\n",
        "print('Total bird records with values in all variables: ', train.dropna().shape[0])\n",
        "print('Total bird records: ', train.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7TOAAhvJ7rGE"
      },
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "train = pd.read_csv('../input/titanic/train.csv')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EF3S6Iab7rGE"
      },
      "cell_type": "code",
      "source": [
        "total = train.isnull().sum().sort_values(ascending=False)\n",
        "percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "f, ax = plt.subplots(figsize=(15, 6))\n",
        "plt.xticks(rotation='90')\n",
        "sns.barplot(x=missing_data.index, y=missing_data['Percent'])\n",
        "plt.xlabel('Features', fontsize=15)\n",
        "plt.ylabel('Percent of Missing Values', fontsize=15)\n",
        "plt.title('Percentage of Missing Data by Feature', fontsize=15)\n",
        "missing_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ToHdnnl47rGE"
      },
      "cell_type": "code",
      "source": [
        "#Let us take Age feature and find the total no of records it has null values\n",
        "df = train\n",
        "df['Age'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "III2UMem7rGE"
      },
      "cell_type": "code",
      "source": [
        "df['Age'].replace(np.NaN,df['Age'].mean()).head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "J_K8iD0F7rGF"
      },
      "cell_type": "code",
      "source": [
        "df_median = train\n",
        "df_median['Age'].fillna(df_median['Age'].median(),inplace=True)\n",
        "df_median['Age']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GXHm-ypu7rGF"
      },
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "train = pd.read_csv('../input/birdsong-recognition/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "B6ZyRc_k7rGG"
      },
      "cell_type": "code",
      "source": [
        "# As per the categorical features which have missing values I have choosen \"background\" as feature for Mode imputation\n",
        "\n",
        "data_cat=train\n",
        "data_cat['background'].fillna(data_cat['background'].mode()[0], inplace=True)\n",
        "data_cat['background'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "189hNPQG7rGG"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "train = pd.read_csv('../input/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_6Lfbx7F7rGG"
      },
      "cell_type": "code",
      "source": [
        "# let's separate into training and testing set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train, train.Survived, test_size=0.3,random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "M9kLvvWH7rGH"
      },
      "cell_type": "code",
      "source": [
        "# let's make a function to create 3 variables from Age 1-filling NA with median, 2- random sampling or 3- zeroes\n",
        "\n",
        "def impute_na(df, variable, median):\n",
        "    df[variable+'_median'] = df[variable].fillna(median)\n",
        "    df[variable+'_zero'] = df[variable].fillna(0)\n",
        "\n",
        "    # random sampling\n",
        "    df[variable+'_random'] = df[variable]\n",
        "    # extract the random sample to fill the na\n",
        "    random_sample = X_train[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)\n",
        "    # pandas needs to have the same index in order to merge datasets\n",
        "    random_sample.index = df[df[variable].isnull()].index\n",
        "    df.loc[df[variable].isnull(), variable+'_random'] = random_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-uqWrTnK7rGH"
      },
      "cell_type": "code",
      "source": [
        "median = X_train.Age.mean()\n",
        "impute_na(X_train, 'Age', median)\n",
        "X_train.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2KnG4Y707rGH"
      },
      "cell_type": "code",
      "source": [
        "# Let us see the distribution of the Age variable after filling NA with random value\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "X_train['Age'].plot(kind='kde', ax=ax)\n",
        "X_train.Age_random.plot(kind='kde', ax=ax, color='red')\n",
        "lines, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(lines, labels, loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pOiC7B5p7rGH"
      },
      "cell_type": "code",
      "source": [
        "# Let us see distribution of the Age variable after filling NA with median value\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "X_train['Age'].plot(kind='kde', ax=ax)\n",
        "X_train.Age_median.plot(kind='kde', ax=ax, color='red')\n",
        "lines, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(lines, labels, loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Zxkw1GX77rGI"
      },
      "cell_type": "code",
      "source": [
        "# Let us see distribution of the Age variable after filling NA with zero value\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "X_train['Age'].plot(kind='kde', ax=ax)\n",
        "X_train.Age_zero.plot(kind='kde', ax=ax, color='red')\n",
        "lines, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(lines, labels, loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "U5R1apJ57rGI"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "train = pd.read_csv('../input/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LAogzZ1R7rGI"
      },
      "cell_type": "code",
      "source": [
        "# let's separate into training and testing set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train, train.Survived, test_size=0.3,random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "edkknP9W7rGI"
      },
      "cell_type": "code",
      "source": [
        "#Let us define a function with replacement of NA value with an arbitrary value as 0 and 100\n",
        "def impute_na(df, variable):\n",
        "    df[variable+'_zero'] = df[variable].fillna(0)\n",
        "    df[variable+'_hundred']= df[variable].fillna(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rk4uhzZ87rGI"
      },
      "cell_type": "code",
      "source": [
        "# let's replace the NA with the median value in the training set\n",
        "impute_na(X_train, 'Age')\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NdDfSSuS7rGJ"
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('../input/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rc0rBD2q7rGJ"
      },
      "cell_type": "code",
      "source": [
        "# let's separate into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(train, train.Survived, test_size=0.3,random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0IYsEmFV7rGJ"
      },
      "cell_type": "code",
      "source": [
        "X_train.Age.hist(bins=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qQgE9QZo7rGJ"
      },
      "cell_type": "code",
      "source": [
        "# far end of the distribution\n",
        "X_train.Age.mean()+3*X_train.Age.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MpZ_k4j_7rGJ"
      },
      "cell_type": "code",
      "source": [
        "# Let us see if there are a few outliers for Age, according to its distribution these outliers will be masked when we replace NA by values at the far end\n",
        "sns.boxplot('Age', data=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HhmqH56W7rGJ"
      },
      "cell_type": "code",
      "source": [
        "def impute_na(df, variable, median, extreme):\n",
        "    df[variable+'_far_end'] = df[variable].fillna(extreme)\n",
        "    df[variable].fillna(median, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "v-CUC8vJ7rGK"
      },
      "cell_type": "code",
      "source": [
        "# let's replace the NA with the median value in the training and testing sets\n",
        "impute_na(X_train, 'Age', X_train.Age.median(), X_train.Age.mean()+3*X_train.Age.std())\n",
        "X_train.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0Ly09xwI7rGQ"
      },
      "cell_type": "code",
      "source": [
        "# As you can see an accumulation of values around the median for the median imputation\n",
        "X_train.Age.hist(bins=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "19n7q0ws7rGQ"
      },
      "cell_type": "code",
      "source": [
        "# Now finally let us see an accumulation of values at the far end imputation\n",
        "X_train.Age_far_end.hist(bins=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3kGsOYBl7rGQ"
      },
      "cell_type": "code",
      "source": [
        "# Far end imputation now indicates that there are no outliers in the variable as shown below\n",
        "sns.boxplot('Age_far_end', data=X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJIVwsOX7rGQ"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://miro.medium.com/max/2560/1*wYbTRM0dgnRzutwZq63xCg.png)\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Dbi4kmTP7rGR"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('../input/titanic/train.csv', usecols=['Sex'])\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JlTREHOR7rGS"
      },
      "cell_type": "code",
      "source": [
        "pd.get_dummies(train_df).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "P70EhgPH7rGS"
      },
      "cell_type": "code",
      "source": [
        "# For a clear understanding let us visualise like below\n",
        "pd.concat([train_df, pd.get_dummies(train_df)], axis=1).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "506lHjTi7rGS"
      },
      "cell_type": "code",
      "source": [
        "pd.get_dummies(train_df, drop_first=True).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0zCzCYYf7rGS"
      },
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv('../input/titanic/train.csv', usecols=['Embarked'])\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qqYwuv0-7rGS"
      },
      "cell_type": "code",
      "source": [
        "# Let us check the number of unique emabrked labels\n",
        "train_df.Embarked.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IntMEJrT7rGT"
      },
      "cell_type": "code",
      "source": [
        "# Now let us get the complete set of dummy variables for embarked feature\n",
        "\n",
        "pd.get_dummies(train_df).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MflJlJCR7rGT"
      },
      "cell_type": "code",
      "source": [
        "# Now let us get k-1 dummy variables\n",
        "\n",
        "pd.get_dummies(train_df, drop_first=True).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "n1nFcrTx7rGT"
      },
      "cell_type": "code",
      "source": [
        "# Also to mention we may have some missing values in this feature so it is better to include an additional dummy variable to indicate whether there was missing data\n",
        "\n",
        "pd.get_dummies(train_df, drop_first=True, dummy_na=True).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "RgrvIbr67rGT"
      },
      "cell_type": "code",
      "source": [
        "# Now let us find out how many observations we have for each variable (i.e., each category)\n",
        "\n",
        "pd.get_dummies(train_df, drop_first=True, dummy_na=True).sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5czTjnpv7rGU"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "R786Lt0f7rGU"
      },
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv('../input/titanic/train.csv')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fdvoQtqr7rGU"
      },
      "cell_type": "code",
      "source": [
        "# Now let us a copy of the above dataset, in which we encode the categorical variables using One Hot Encoder\n",
        "\n",
        "train_df_OneHotEncoder = pd.concat([train_df[['Pclass', 'Age', 'SibSp','Parch', 'Survived']], # Choosen the numerical variables\n",
        "                      pd.get_dummies(train_df.Sex, drop_first=True),   # Sex as explained above which is binary categorical variable\n",
        "                      pd.get_dummies(train_df.Embarked, drop_first=True)],  # Embarked as explained above has k categories in categorical\n",
        "                    axis=1)\n",
        "\n",
        "train_df_OneHotEncoder.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fTwHk62f7rGU"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_df_OneHotEncoder[['Pclass', 'Age', 'SibSp',\n",
        "                                                              'Parch', 'male', 'Q', 'S']].fillna(0),\n",
        "                                                    train_df_OneHotEncoder.Survived,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zxFpZMbL7rGV"
      },
      "cell_type": "code",
      "source": [
        "# let's build a random forest model with the above data\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=3)\n",
        "rf.fit(X_train, y_train)\n",
        "print('Train set')\n",
        "pred = rf.predict_proba(X_train)\n",
        "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
        "print('Test set')\n",
        "pred = rf.predict_proba(X_test)\n",
        "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PE8xuXqq7rGV"
      },
      "cell_type": "code",
      "source": [
        "# Now let us build a AdaBoost classifier\n",
        "\n",
        "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
        "ada.fit(X_train, y_train)\n",
        "print('Train set')\n",
        "pred = ada.predict_proba(X_train)\n",
        "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
        "print('Test set')\n",
        "pred = ada.predict_proba(X_test)\n",
        "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xUjMlt9N7rGV"
      },
      "cell_type": "code",
      "source": [
        "# Finally with logistic regression\n",
        "logit = LogisticRegression(random_state=44)\n",
        "logit.fit(X_train, y_train)\n",
        "print('Train set')\n",
        "pred = logit.predict_proba(X_train)\n",
        "print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
        "print('Test set')\n",
        "pred = logit.predict_proba(X_test)\n",
        "print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VWmgHxkE7rGV"
      },
      "cell_type": "code",
      "source": [
        "# let's load the variable Cabin of the titanic dataset\n",
        "\n",
        "train_df=pd.read_csv('../input/titanic/train.csv', usecols = ['Cabin'])\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7bs_EvmY7rGW"
      },
      "cell_type": "code",
      "source": [
        "# Now let's inspect the number of unique labels in Cabin feature\n",
        "print('Number of unique labels in Cabin Feature: {}'.format(len(train_df.Cabin.unique())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "p3W-3O7N7rGW"
      },
      "cell_type": "code",
      "source": [
        "# Now let us see how many features we can create if we did One Hot Encoder for Cabin feature\n",
        "Cabin_OneHotEncoder = pd.get_dummies(train_df.Cabin)\n",
        "Cabin_OneHotEncoder.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tFaNHQSK7rGW"
      },
      "cell_type": "code",
      "source": [
        "Cabin_OneHotEncoder.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "geOH8x-17rGW"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('../input/titanic/train.csv', usecols=['Embarked', 'Survived'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "djuMFjG07rGW"
      },
      "cell_type": "code",
      "source": [
        "# let's have a look at how many labels\n",
        "\n",
        "for col in data.columns[1:]:\n",
        "    print(col, ': ', len(data[col].unique()), ' labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xTfFg2Bh7rGX"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[['Embarked']], data.Survived,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rG2Kw3bH7rGX"
      },
      "cell_type": "code",
      "source": [
        "# Let's obtain the counts for each one of the labels in variable Embarked and capture this in a dictionary that we can use to re-map the labels\n",
        "\n",
        "X_train.Embarked.value_counts().to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "n0OR2psB7rGX"
      },
      "cell_type": "code",
      "source": [
        "# lets look at X_train so we can compare then the variable re-coding\n",
        "\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "q7Ydp-qy7rGX"
      },
      "cell_type": "code",
      "source": [
        "# And now let's replace each label in X2 by its count.Firstly we make a dictionary that maps each label to the counts\n",
        "X_frequency_map = X_train.Embarked.value_counts().to_dict()\n",
        "\n",
        "# and now we replace X2 labels both in train and test set with the same map\n",
        "X_train.Embarked = X_train.Embarked.map(X_frequency_map)\n",
        "X_test.Embarked = X_test.Embarked.map(X_frequency_map)\n",
        "\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nwwX098K7rGX"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import category_encoders as ce\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "df = pd.read_csv('../input/titanic/train.csv', usecols=['Embarked', 'Survived'])\n",
        "X = df.drop('Survived', axis = 1)\n",
        "y = df.drop('Embarked', axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hAXlhNpF7rGY"
      },
      "cell_type": "code",
      "source": [
        "binary_encoder = ce.BinaryEncoder(cols = ['Embarked'])\n",
        "binary_encoder.fit_transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "BfwJQcpy7rGY"
      },
      "cell_type": "code",
      "source": [
        "ordinal_encoder = ce.OrdinalEncoder(cols = ['Embarked'])\n",
        "ordinal_encoder.fit_transform(X, y['Survived'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wmE3q1uy7rGY"
      },
      "cell_type": "code",
      "source": [
        "BaseN_encoder = ce.BaseNEncoder(cols = ['Embarked'])\n",
        "BaseN_encoder.fit_transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dTpxDjMd7rGZ"
      },
      "cell_type": "code",
      "source": [
        "Hashing_encoder = ce.HashingEncoder(cols = ['Embarked'])\n",
        "Hashing_encoder.fit_transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-dbwbCaB7rGZ"
      },
      "cell_type": "code",
      "source": [
        "Sum_encoder = ce.SumEncoder(cols = ['Embarked'])\n",
        "Sum_encoder.fit_transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gTu6DZvd7rGa"
      },
      "cell_type": "code",
      "source": [
        "ce_helmert = ce.HelmertEncoder(cols = ['Embarked'])\n",
        "ce_helmert.fit_transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WL9mZPtv7rGa"
      },
      "cell_type": "code",
      "source": [
        "ce_backward = ce.BackwardDifferenceEncoder(cols = ['Embarked'])\n",
        "ce_backward.fit_transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "902a2Nft7rGa"
      },
      "cell_type": "code",
      "source": [
        "ce_poly = ce.PolynomialEncoder(cols = ['Embarked'])\n",
        "ce_poly.fit_transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6Igao5ru7rGa"
      },
      "cell_type": "code",
      "source": [
        "ce_target = ce.TargetEncoder(cols = ['Embarked'])\n",
        "ce_target.fit(X, y)\n",
        "ce_target.transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zA77gIwC7rGb"
      },
      "cell_type": "code",
      "source": [
        "# Target with higher smoothing\n",
        "ce_target_leaf = ce.TargetEncoder(cols = ['Embarked'], smoothing = 10)\n",
        "ce_target_leaf.fit(X, y)\n",
        "ce_target_leaf.transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KSiDGFpW7rGb"
      },
      "cell_type": "code",
      "source": [
        "# Target with lower smoothing\n",
        "ce_target_leaf = ce.TargetEncoder(cols = ['Embarked'], smoothing = .10)\n",
        "ce_target_leaf.fit(X, y)\n",
        "ce_target_leaf.transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "D4v2dHCP7rGb"
      },
      "cell_type": "code",
      "source": [
        "!pip install chart-studio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "trusted": true,
        "id": "kcZzFsbv7rGb"
      },
      "cell_type": "code",
      "source": [
        "!pip install woe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "svDG9M4t7rGc"
      },
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas.core.algorithms as algos\n",
        "from pandas import Series\n",
        "import scipy.stats.stats as stats\n",
        "import re\n",
        "import traceback\n",
        "import string\n",
        "import os\n",
        "import woe\n",
        "from woe.eval import plot_ks\n",
        "print(os.listdir(\"../input\"))\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 14, 8\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import chart_studio.plotly.plotly as py\n",
        "import chart_studio.plotly\n",
        "max_bin = 20\n",
        "force_bin = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uJhLy8d37rGc"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../input/uci-credit-carefrom-python-woe-pkg/UCI_Credit_Card.csv',sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4UbVZC9F7rGc"
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QODUzMlk7rGc"
      },
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "krQisxWO7rGc"
      },
      "cell_type": "markdown",
      "source": [
        "Define a binning function for continuous independent variables"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-fub-Z7h7rGd"
      },
      "cell_type": "code",
      "source": [
        "def mono_bin(Y, X, n = max_bin):\n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
        "    r = 0\n",
        "    while np.abs(r) < 1:\n",
        "        try:\n",
        "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
        "            d2 = d1.groupby('Bucket', as_index=True)\n",
        "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
        "            n = n - 1\n",
        "        except Exception as e:\n",
        "            n = n - 1\n",
        "\n",
        "    if len(d2) == 1:\n",
        "        n = force_bin\n",
        "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
        "        if len(np.unique(bins)) == 2:\n",
        "            bins = np.insert(bins, 0, 1)\n",
        "            bins[1] = bins[1]-(bins[1]/2)\n",
        "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)})\n",
        "        d2 = d1.groupby('Bucket', as_index=True)\n",
        "\n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"MIN_VALUE\"] = d2.min().X\n",
        "    d3[\"MAX_VALUE\"] = d2.max().X\n",
        "    d3[\"COUNT\"] = d2.count().Y\n",
        "    d3[\"EVENT\"] = d2.sum().Y\n",
        "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
        "    d3=d3.reset_index(drop=True)\n",
        "\n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "\n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]\n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "\n",
        "    return(d3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xupxprK_7rGd"
      },
      "cell_type": "code",
      "source": [
        "def char_bin(Y, X):\n",
        "\n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
        "    df2 = notmiss.groupby('X',as_index=True)\n",
        "\n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"COUNT\"] = df2.count().Y\n",
        "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
        "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
        "    d3[\"EVENT\"] = df2.sum().Y\n",
        "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
        "\n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "\n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]\n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    d3 = d3.reset_index(drop=True)\n",
        "\n",
        "    return(d3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "K0Mqz7jo7rGd"
      },
      "cell_type": "code",
      "source": [
        "def data_vars(df1, target):\n",
        "\n",
        "    stack = traceback.extract_stack()\n",
        "    filename, lineno, function_name, code = stack[-2]\n",
        "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
        "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
        "\n",
        "    x = df1.dtypes.index\n",
        "    count = -1\n",
        "\n",
        "    for i in x:\n",
        "        if i.upper() not in (final.upper()):\n",
        "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
        "                conv = mono_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i\n",
        "                count = count + 1\n",
        "            else:\n",
        "                conv = char_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i\n",
        "                count = count + 1\n",
        "\n",
        "            if count == 0:\n",
        "                iv_df = conv\n",
        "            else:\n",
        "                iv_df = iv_df.append(conv,ignore_index=True)\n",
        "\n",
        "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
        "    iv = iv.reset_index()\n",
        "    return(iv_df,iv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XgAx7IyW7rGe"
      },
      "cell_type": "code",
      "source": [
        "final_iv, IV = data_vars(df,df.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LWMg0Ryc7rGf"
      },
      "cell_type": "code",
      "source": [
        "final_iv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1FhvDih77rGf"
      },
      "cell_type": "code",
      "source": [
        "IV.sort_values('IV',ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "y244KR_17rGf"
      },
      "cell_type": "code",
      "source": [
        "# Using the category encoder library\n",
        "ce_WOE = ce.WOEEncoder(cols = ['Embarked'])\n",
        "ce_WOE.fit(X, y)\n",
        "ce_WOE.transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Saj1oGha7rGf"
      },
      "cell_type": "code",
      "source": [
        "# let's load again the titanic dataset\n",
        "\n",
        "data = pd.read_csv('../input/titanic/train.csv', usecols=['Cabin', 'Survived'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "a1rHUCyd7rGf"
      },
      "cell_type": "code",
      "source": [
        "# let's first fill NA values with an additional label\n",
        "\n",
        "data.Cabin.fillna('Missing', inplace=True)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "i4xogPzB7rGg"
      },
      "cell_type": "code",
      "source": [
        "# Cabin has indeed a lot of labels, here for simplicity, I will capture the first letter of the cabin, but the procedure could be done as well without any prior variable manipulation\n",
        "\n",
        "len(data.Cabin.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xok_RPN47rGg"
      },
      "cell_type": "code",
      "source": [
        "# Now we extract the first letter of the cabin\n",
        "data['Cabin'] = data['Cabin'].astype(str).str[0]\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8fCcDxhq7rGg"
      },
      "cell_type": "code",
      "source": [
        "# check the labels\n",
        "data.Cabin.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kB95L8Fm7rGg"
      },
      "cell_type": "code",
      "source": [
        "#The calculation of the WoE to replace the labels should be done considering the ONLY the training set, and then expanded it to the test set.\n",
        "# Let's divide into train and test set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Cabin', 'Survived']], data.Survived, test_size=0.3,\n",
        "                                                    random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8na4yZNM7rGg"
      },
      "cell_type": "code",
      "source": [
        "# Now we calculate the probability of target=1\n",
        "X_train.groupby(['Cabin'])['Survived'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rRxT9mlD7rGh"
      },
      "cell_type": "code",
      "source": [
        "# Let's make a dataframe with the above calculation\n",
        "\n",
        "prob_df = X_train.groupby(['Cabin'])['Survived'].mean()\n",
        "prob_df = pd.DataFrame(prob_df)\n",
        "prob_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fYnIACwB7rGh"
      },
      "cell_type": "code",
      "source": [
        "# and now the probability of target = 0 and we add it to the dataframe\n",
        "\n",
        "prob_df = X_train.groupby(['Cabin'])['Survived'].mean()\n",
        "prob_df = pd.DataFrame(prob_df)\n",
        "prob_df['Died'] = 1-prob_df.Survived\n",
        "prob_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "edud4ruG7rGh"
      },
      "cell_type": "code",
      "source": [
        "# Since the log of zero is not defined, let's set this number to something small and non-zero\n",
        "\n",
        "prob_df.loc[prob_df.Survived == 0, 'Survived'] = 0.00001\n",
        "prob_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zOXkr5DI7rGh"
      },
      "cell_type": "code",
      "source": [
        "# Finally it is time to  calculate the Weight of Evidence (WoE)\n",
        "\n",
        "prob_df['WoE'] = np.log(prob_df.Survived/prob_df.Died)\n",
        "prob_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "imK9PdOx7rGh"
      },
      "cell_type": "code",
      "source": [
        "# Let us create a dictionary to re-map the variable\n",
        "\n",
        "prob_df['WoE'].to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fjmMxLO47rGi"
      },
      "cell_type": "code",
      "source": [
        "# Now we make a dictionary to map the orignal variable to the WoE but we capture the dictionary in a variable\n",
        "\n",
        "ordered_labels = prob_df['WoE'].to_dict()\n",
        "# Replace the labels with the above label for WoE\n",
        "\n",
        "X_train['Cabin_ordered'] = X_train.Cabin.map(ordered_labels)\n",
        "X_test['Cabin_ordered'] = X_test.Cabin.map(ordered_labels)\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Mro0dbpa7rGi"
      },
      "cell_type": "code",
      "source": [
        "# Plot the original variable\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "fig = plt.figure()\n",
        "fig = X_train.groupby(['Cabin'])['Survived'].mean().plot()\n",
        "fig.set_title('Normal relationship between variable and target')\n",
        "fig.set_ylabel('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6Qv_KIHH7rGi"
      },
      "cell_type": "code",
      "source": [
        "# Plot the transformed result: the monotonic variable\n",
        "\n",
        "fig = plt.figure()\n",
        "fig = X_train.groupby(['Cabin_ordered'])['Survived'].mean().plot()\n",
        "fig.set_title('Monotonic relationship between variable and target')\n",
        "fig.set_ylabel('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2a0CLwPD7rGi"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "n8T_P2lS7rGj"
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('../input/titanic/train.csv', usecols=['Embarked', 'Survived'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LB1N1GX37rGj"
      },
      "cell_type": "code",
      "source": [
        "# let's check at the different number of labels within each variable\n",
        "cols_to_use = ['Embarked']\n",
        "\n",
        "for col in cols_to_use:\n",
        "    print('Variable: ', col, ' Number of Labels: ', len(data[col].unique()))\n",
        "\n",
        "print('Total passengers: ', len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "oNzN2USB7rGj"
      },
      "cell_type": "code",
      "source": [
        "# let's plot how frequently appears each label within a variable in the dataset\n",
        "\n",
        "total_passengers = len(data)\n",
        "\n",
        "for col in cols_to_use:\n",
        "    # count the number of observations per label and divide by total\n",
        "    # number of cars\n",
        "    temp_df = pd.Series(data[col].value_counts() / total_passengers)\n",
        "\n",
        "    # make plot with the above percentages\n",
        "    fig = temp_df.sort_values(ascending=False).plot.bar()\n",
        "    fig.set_xlabel(col)\n",
        "    fig.set_ylabel('Percentage of Passengers')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DB9-Jk9j7rGj"
      },
      "cell_type": "code",
      "source": [
        "# I will work first the the variable Embarked\n",
        "\n",
        "# Let's calculate again the frequency of the different categories/labels in Embarked\n",
        "\n",
        "temp_df = pd.Series(data['Embarked'].value_counts() / total_passengers).reset_index()\n",
        "temp_df.columns = ['Embarked', 'Percentage of Passengers']\n",
        "temp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "68ATSEqS7rGj"
      },
      "cell_type": "code",
      "source": [
        "# Now let's calculate the mean \"time to pass testing\" for each label in Embarked\n",
        "\n",
        "data.groupby(['Embarked'])['Survived'].mean().reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gEHxIs9_7rGk"
      },
      "cell_type": "code",
      "source": [
        "ce_leave = ce.LeaveOneOutEncoder(cols = ['Embarked'])\n",
        "ce_leave.fit(X, y)\n",
        "ce_leave.transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Z6Wexr667rGk"
      },
      "cell_type": "code",
      "source": [
        "ce_James = ce.JamesSteinEncoder(cols = ['Embarked'])\n",
        "ce_James.fit(X, y)\n",
        "ce_James.transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mWE1I3z77rGl"
      },
      "cell_type": "code",
      "source": [
        "ce_M_Estimator = ce.MEstimateEncoder(cols = ['Embarked'])\n",
        "ce_M_Estimator.fit(X, y)\n",
        "ce_M_Estimator.transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-R3Vbaqy7rGl"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pylab\n",
        "import scipy.stats as stats\n",
        "# load the numerical variables of the Titanic Dataset\n",
        "\n",
        "train_data = pd.read_csv('../input/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PWHY57XC7rGm"
      },
      "cell_type": "code",
      "source": [
        "# first fill the missing data of the variable age, with a random sample of the variable\n",
        "\n",
        "def impute_na(data, variable):\n",
        "    # function to fill na with a random sample\n",
        "    df = data.copy()\n",
        "\n",
        "    # random sampling\n",
        "    df[variable+'_random'] = df[variable]\n",
        "\n",
        "    # extract the random sample to fill the na\n",
        "    random_sample = df[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)\n",
        "\n",
        "    # pandas needs to have the same index in order to merge datasets\n",
        "    random_sample.index = df[df[variable].isnull()].index\n",
        "    df.loc[df[variable].isnull(), variable+'_random'] = random_sample\n",
        "\n",
        "    return df[variable+'_random']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nQTXXq5H7rGm"
      },
      "cell_type": "code",
      "source": [
        "# fill nul values for Age\n",
        "train_data['Age'] = impute_na(train_data, 'Age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HHJFf1_T7rGm"
      },
      "cell_type": "code",
      "source": [
        "# Plot the histograms to have a quick look at the distributions and  plot Q-Q plots to visualise if the variable is normally distributed\n",
        "\n",
        "def diagnostic_plots(df, variable):\n",
        "    # function to plot a histogram and a Q-Q plot\n",
        "    # side by side, for a certain variable\n",
        "\n",
        "    plt.figure(figsize=(15,6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    df[variable].hist()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=pylab)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "diagnostic_plots(train_data, 'Age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ezmouLqS7rGn"
      },
      "cell_type": "code",
      "source": [
        "train_data['Age_log'] = np.log(train_data.Age)\n",
        "diagnostic_plots(train_data, 'Age_log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xP0NDZ4w7rGn"
      },
      "cell_type": "code",
      "source": [
        "train_data['Age_reciprocal'] = 1 / train_data.Age\n",
        "diagnostic_plots(train_data, 'Age_reciprocal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lCjvYjvN7rGn"
      },
      "cell_type": "code",
      "source": [
        "train_data['Age_sqr'] =train_data.Age**(1/2)\n",
        "diagnostic_plots(train_data, 'Age_sqr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5-61km5_7rGo"
      },
      "cell_type": "code",
      "source": [
        "train_data['Age_exp'] = train_data.Age**(1/1.2)\n",
        "diagnostic_plots(train_data, 'Age_exp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-QAsg5Fn7rGo"
      },
      "cell_type": "code",
      "source": [
        "train_data['Age_yeojohnson'], param = stats.yeojohnson(train_data.Age)\n",
        "print('Optimal λ: ', param)\n",
        "diagnostic_plots(train_data, 'Age_yeojohnson')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "80f92fzv7rGo"
      },
      "cell_type": "code",
      "source": [
        "train_data['Age_boxcox'], param = stats.boxcox(train_data.Age)\n",
        "print('Optimal λ: ', param)\n",
        "diagnostic_plots(train_data, 'Age_boxcox')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kqBO5OmC7rGq"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "equal_width_data = pd.read_csv('../input/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])\n",
        "equal_width_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mHuHOG1t7rGq"
      },
      "cell_type": "code",
      "source": [
        "# Let's separate into train and test set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(equal_width_data[['Age', 'Fare', 'Survived']], equal_width_data.Survived, test_size=0.3,\n",
        "                                                    random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FMlf3gIs7rGq"
      },
      "cell_type": "code",
      "source": [
        "# replace NA in both  train and test sets\n",
        "\n",
        "X_train['Age'] = impute_na(equal_width_data, 'Age')\n",
        "X_test['Age'] = impute_na(equal_width_data, 'Age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nHLnzOGm7rGq"
      },
      "cell_type": "code",
      "source": [
        "# Let us remind ourselves of the distribution of Age\n",
        "equal_width_data.Age.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HuyhHhbs7rGr"
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "fig = equal_width_data.groupby(['Age'])['Survived'].mean().plot()\n",
        "fig.set_title('Normal relationship between Age and Survived')\n",
        "fig.set_ylabel('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nRa2HUT27rGr"
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "fig = equal_width_data.groupby(['Age'])['Survived'].count().plot()\n",
        "fig.set_title('Number of people per year age bin')\n",
        "fig.set_ylabel('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "setpaKfh7rGr"
      },
      "cell_type": "code",
      "source": [
        "# Let us capture the range of the variable age to begin with\n",
        "\n",
        "Age_range = X_train.Age.max() - X_train.Age.min()\n",
        "Age_range"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dWcEWJRJ7rGr"
      },
      "cell_type": "code",
      "source": [
        "# Now let us capture the lower and upper boundaries\n",
        "\n",
        "min_value = int(np.floor(X_train.Age.min()))\n",
        "max_value = int(np.ceil(X_train.Age.max()))\n",
        "\n",
        "# let's round the bin width\n",
        "inter_value = int(np.round(Age_range/10))\n",
        "\n",
        "min_value, max_value, inter_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lh0Itaus7rGr"
      },
      "cell_type": "code",
      "source": [
        "# Let us capture the interval limits, so we can pass them to the pandas cut function to generate the bins\n",
        "\n",
        "intervals = [i for i in range(min_value, max_value+inter_value, inter_value)]\n",
        "intervals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XAXxh7iF7rGs"
      },
      "cell_type": "code",
      "source": [
        "# let's make labels to label the different bins\n",
        "labels = ['Bin_'+str(i) for i in range(1,len(intervals))]\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "A7-K9TuG7rGs"
      },
      "cell_type": "code",
      "source": [
        "# create Binned age groups\n",
        "\n",
        "# create one column with labels\n",
        "X_train['Age_disc_labels'] = pd.cut(x = X_train.Age, bins=intervals, labels=labels, include_lowest=True)\n",
        "\n",
        "# and one with bin boundaries\n",
        "X_train['Age_disc'] = pd.cut(x = X_train.Age, bins=intervals, include_lowest=True)\n",
        "\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bEmUoiyB7rGs"
      },
      "cell_type": "code",
      "source": [
        "X_train.groupby('Age_disc')['Age'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lMHEEkYM7rGs"
      },
      "cell_type": "code",
      "source": [
        "X_train.groupby('Age_disc')['Age'].count().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "waxui7_-7rGs"
      },
      "cell_type": "code",
      "source": [
        "X_test['Age_disc_labels'] = pd.cut(x = X_test.Age, bins=intervals, labels=labels, include_lowest=True)\n",
        "X_test['Age_disc'] = pd.cut(x = X_test.Age, bins=intervals,  include_lowest=True)\n",
        "\n",
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "i-CC9cZ37rGt"
      },
      "cell_type": "code",
      "source": [
        "# If the distributions in train and test set are similar, we should expect similar distribution of observations in the different intervals in the train and test set.\n",
        "\n",
        "t1 = X_train.groupby(['Age_disc'])['Survived'].count() / np.float(len(X_train))\n",
        "t2 = X_test.groupby(['Age_disc'])['Survived'].count() / np.float(len(X_test))\n",
        "temp = pd.concat([t1,t2], axis=1)\n",
        "temp.columns = ['train', 'test']\n",
        "temp.plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "g8qPrpfN7rGt"
      },
      "cell_type": "code",
      "source": [
        "# Now let's observe the relationship between age and surival again, using the discrete Age transformed variable\n",
        "\n",
        "fig = plt.figure()\n",
        "fig = X_train.groupby(['Age_disc'])['Survived'].mean().plot(figsize=(12,6))\n",
        "fig.set_title('Normal relationship between variable and target')\n",
        "fig.set_ylabel('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rbWWMg8y7rGt"
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "fig = X_train.groupby(['Age_disc'])['Survived'].count().plot(figsize=(12,6))\n",
        "fig.set_title('Number of Passengers within each Age bin')\n",
        "fig.set_ylabel('No of Passengers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "b-pjUyhp7rGt"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "equal_freq_data= pd.read_csv('../input/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])\n",
        "equal_freq_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NQPDAHqC7rGv"
      },
      "cell_type": "code",
      "source": [
        "# Let's separate into train and test set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(equal_freq_data[['Age', 'Fare', 'Survived']],equal_freq_data.Survived, test_size=0.3, random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KLWeP6C07rGv"
      },
      "cell_type": "code",
      "source": [
        "# replace NA in both train and test sets\n",
        "\n",
        "X_train['Age'] = impute_na(equal_freq_data, 'Age')\n",
        "X_test['Age'] = impute_na(equal_freq_data, 'Age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Y4REQr4-7rGw"
      },
      "cell_type": "code",
      "source": [
        "# let's remind ourselves of the original distribution\n",
        "\n",
        "equal_freq_data.Age.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "91sEQIbE7rGw"
      },
      "cell_type": "code",
      "source": [
        "# We will divide Age in 5 bins. I use the qcut (quantile cut) function from pandas and I indicate that I want 4 cutting points, thus 5 bins. retbins= True indicates that I want to capture the limits of each interval (so I can then use them to cut the test set)\n",
        "\n",
        "Age_disccretised, intervals = pd.qcut(equal_freq_data.Age, 4, labels=None, retbins=True, precision=3, duplicates='raise')\n",
        "pd.concat([Age_disccretised, equal_freq_data.Age], axis=1).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PVT8IuUY7rGx"
      },
      "cell_type": "code",
      "source": [
        "intervals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xI9G9bUQ7rGx"
      },
      "cell_type": "code",
      "source": [
        "# Calculate number of passengers per bin\n",
        "temp = pd.concat([Age_disccretised, equal_freq_data.Age], axis=1)\n",
        "temp.columns = ['Age_discretised', 'Age']\n",
        "temp.groupby('Age_discretised')['Age'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dj85UhoW7rGx"
      },
      "cell_type": "code",
      "source": [
        "# We can also add labels instead of having the interval boundaries, to the bins, as follows:\n",
        "\n",
        "Age_disccretised, intervals = pd.qcut(equal_freq_data.Age, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'], retbins=True, precision=3, duplicates='raise')\n",
        "Age_disccretised.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "A70T_QGI7rGx"
      },
      "cell_type": "code",
      "source": [
        "# We will divide into 10 quantiles for the rest of the exercise. I will leave the quantile boundary and generate labels as well for comparison\n",
        "\n",
        "# create 10 labels, one for each quantile\n",
        "\n",
        "labels = ['Q'+str(i+1) for i in range(0,10)]\n",
        "print(labels)\n",
        "\n",
        "# bins with labels\n",
        "X_train['Age_disc_label'], bins = pd.qcut(x=X_train.Age, q=10, labels=labels, retbins=True, precision=3, duplicates='raise')\n",
        "\n",
        "# bins with boundaries\n",
        "X_train['Age_disc'], bins = pd.qcut(x=X_train.Age, q=10, retbins=True, precision=3, duplicates='raise')\n",
        "\n",
        "\n",
        "X_train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TLdMweYU7rGy"
      },
      "cell_type": "code",
      "source": [
        "X_test['Age_disc_label'] = pd.cut(x = X_test.Age, bins=bins, labels=labels)\n",
        "X_test['Age_disc'] = pd.cut(x = X_test.Age, bins=bins)\n",
        "\n",
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "i2Kc46157rGy"
      },
      "cell_type": "code",
      "source": [
        "# let's check that we have equal frequency (equal number of observations per bin)\n",
        "X_test.groupby('Age_disc')['Age'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8ZcA1i_F7rGy"
      },
      "cell_type": "code",
      "source": [
        "t1 = X_train.groupby(['Age_disc'])['Survived'].count() / np.float(len(X_train))\n",
        "t1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tpg4qv4p7rGy"
      },
      "cell_type": "code",
      "source": [
        "t2 = X_test.groupby(['Age_disc'])['Survived'].count() / np.float(len(X_test))\n",
        "t2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kni5ayIf7rGy"
      },
      "cell_type": "code",
      "source": [
        "temp = pd.concat([t1,t2], axis=1)\n",
        "temp.columns = ['train', 'test']\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dzmByHOG7rGz"
      },
      "cell_type": "code",
      "source": [
        "temp.plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rz4UxT5U7rGz"
      },
      "cell_type": "code",
      "source": [
        "# Let's observe the relationship between age and survival again, using the discrete Age transformed variable\n",
        "\n",
        "fig = plt.figure()\n",
        "fig = X_train.groupby(['Age_disc'])['Survived'].mean().plot(figsize=(12,6))\n",
        "fig.set_title('Normal relationship between variable and target')\n",
        "fig.set_ylabel('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ovoAzKmr7rGz"
      },
      "cell_type": "code",
      "source": [
        "# order the intervals according to survival rate\n",
        "ordered_labels = X_train.groupby(['Age_disc_label'])['Survived'].mean().sort_values().index\n",
        "\n",
        "# number the intervals according to survival rate\n",
        "ordinal_label = {k:i for i, k in enumerate(ordered_labels, 0)}\n",
        "\n",
        "# remap the intervals to the encoded variable\n",
        "X_train['Age_disc_ordered'] = X_train.Age_disc_label.map(ordinal_label)\n",
        "X_test['Age_disc_ordered'] = X_test.Age_disc_label.map(ordinal_label)\n",
        "\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "i68TZQdw7rGz"
      },
      "cell_type": "code",
      "source": [
        "# Plot the monotonic relationship\n",
        "fig = plt.figure()\n",
        "fig = X_train.groupby(['Age_disc_ordered'])['Survived'].mean().plot()\n",
        "fig.set_title('Monotonic relationship between discretised Age and target')\n",
        "fig.set_ylabel('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-RT0jpAr7rGz"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# load the numerical variables of the Titanic Dataset\n",
        "data_decision_tree = pd.read_csv('../input/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])\n",
        "data_decision_tree.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "76KWto0O7rGz"
      },
      "cell_type": "code",
      "source": [
        "# Let's separate into train and test set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_decision_tree[['Age', 'Fare', 'Survived']],\n",
        "                                                    data_decision_tree.Survived, test_size=0.3,\n",
        "                                                    random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8KCfEcCa7rG0"
      },
      "cell_type": "code",
      "source": [
        "X_train['Age'] = impute_na(data_decision_tree, 'Age')\n",
        "X_test['Age'] = impute_na(data_decision_tree, 'Age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tB6x17O47rG0"
      },
      "cell_type": "code",
      "source": [
        "# Let us now  build Classification tree using Age to predict Survived\n",
        "\n",
        "tree_model = DecisionTreeClassifier(max_depth=2)\n",
        "tree_model.fit(X_train.Age.to_frame(), X_train.Survived)\n",
        "X_train['Age_tree'] = tree_model.predict_proba(X_train.Age.to_frame())[:,1]\n",
        "X_train.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Yka-F4w97rG0"
      },
      "cell_type": "code",
      "source": [
        "# monotonic relationship with target\n",
        "\n",
        "fig = plt.figure()\n",
        "fig = X_train.groupby(['Age_tree'])['Survived'].mean().plot()\n",
        "fig.set_title('Monotonic relationship between discretised Age and target')\n",
        "fig.set_ylabel('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4kRS5LL_7rG0"
      },
      "cell_type": "code",
      "source": [
        "# Number of passengers per probabilistic bucket / bin\n",
        "\n",
        "X_train.groupby(['Age_tree'])['Survived'].count().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KBzr384T7rG0"
      },
      "cell_type": "code",
      "source": [
        "# Median age within each bucket originated by the tree\n",
        "\n",
        "X_train.groupby(['Age_tree'])['Age'].median().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tvpOkpyM7rG0"
      },
      "cell_type": "code",
      "source": [
        "# Now let us see the Age limits buckets generated by the tree by capturing the minimum and maximum age per each probability bucket, we get an idea of the bucket cut-offs\n",
        "\n",
        "pd.concat( [X_train.groupby(['Age_tree'])['Age'].min(),\n",
        "            X_train.groupby(['Age_tree'])['Age'].max()], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_oZlBWLv7rG1"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# To display the total number columns present in the dataset\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "INBX692Q7rG1"
      },
      "cell_type": "code",
      "source": [
        "# Let us load the titanic dataset\n",
        "\n",
        "data = pd.read_csv('../input/titanic/train.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lEWe6Q3T7rG1"
      },
      "cell_type": "code",
      "source": [
        "data.Age.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vEUOsUR_7rG1"
      },
      "cell_type": "code",
      "source": [
        "Upper_boundary_limit = data.Age.mean() + 3* data.Age.std()\n",
        "Lower_boundary_limit = data.Age.mean() - 3* data.Age.std()\n",
        "\n",
        "Upper_boundary_limit, Lower_boundary_limit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "50bWCgSW7rG2"
      },
      "cell_type": "code",
      "source": [
        "IQR = data.Age.quantile(0.75) - data.Age.quantile(0.25)\n",
        "\n",
        "Lower_quantile_lower = data.Age.quantile(0.25) - (IQR * 1.5)\n",
        "Upper_quantile_lower = data.Age.quantile(0.75) + (IQR * 1.5)\n",
        "\n",
        "Upper_quantile_lower, Lower_quantile_lower, IQR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wPIdJp1h7rG2"
      },
      "cell_type": "code",
      "source": [
        "IQR = data.Age.quantile(0.75) - data.Age.quantile(0.25)\n",
        "\n",
        "Lower_quantile = data.Age.quantile(0.25) - (IQR * 3)\n",
        "Upper_quantile = data.Age.quantile(0.75) + (IQR * 3)\n",
        "\n",
        "Upper_quantile, Lower_quantile, IQR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "M0lmvhU57rG2"
      },
      "cell_type": "code",
      "source": [
        "data = data.dropna(subset=['Age'])\n",
        "\n",
        "total_passengers = np.float(data.shape[0])\n",
        "\n",
        "print('Passengers older than 73 years old (Gaussian approach): {}'.format(data[data.Age > 73].shape[0] / total_passengers))\n",
        "print('Passengers older than 65 years (IQR): {}'.format(data[data.Age > 65].shape[0] / total_passengers))\n",
        "print('Passengers older than 91 years (IQR, extreme): {}'.format(data[data.Age >= 91].shape[0] / total_passengers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ti4RdF4A7rG3"
      },
      "cell_type": "code",
      "source": [
        "data[(data.Age<Lower_quantile_lower)|(data.Age>Upper_quantile_lower)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Xxyuf2DN7rG3"
      },
      "cell_type": "code",
      "source": [
        "data_with_no_outlier = data[(data.Age>Lower_quantile_lower)&(data.Age<Upper_quantile_lower)]\n",
        "data_with_no_outlier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DKrTOXGv7rG3"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3hiJGVTt7rG4"
      },
      "cell_type": "code",
      "source": [
        "upper_threshold = df['SalePrice'].quantile(0.95)\n",
        "upper_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EZ49W7l57rG4"
      },
      "cell_type": "code",
      "source": [
        "lower_threshold = df['SalePrice'].quantile(0.05)\n",
        "lower_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gt4Tg9987rG4"
      },
      "cell_type": "code",
      "source": [
        "df[(df.SalePrice<lower_threshold)|(df.SalePrice>upper_threshold)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Ks8TKU1Q7rG5"
      },
      "cell_type": "code",
      "source": [
        "data_with_no_outlier_percentile_approach = df[(df.SalePrice>lower_threshold)&(df.SalePrice<upper_threshold)]\n",
        "data_with_no_outlier_percentile_approach"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pEyBB2v97rG5"
      },
      "cell_type": "code",
      "source": [
        "data_zscore = pd.read_csv('../input/titanic/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YqzXWjCv7rG5"
      },
      "cell_type": "code",
      "source": [
        "data_zscore.Age.mean(),data_zscore.Age.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Kj-t-TKD7rG6"
      },
      "cell_type": "code",
      "source": [
        "data_zscore['zscore'] = ( data_zscore.Age - data_zscore.Age.mean() ) / data_zscore.Age.std()\n",
        "data_zscore.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IGCoBM-o7rG6"
      },
      "cell_type": "code",
      "source": [
        "(22-29.69)/14.509433962264152"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pDTOkd3F7rG6"
      },
      "cell_type": "code",
      "source": [
        "data_zscore[(data_zscore.zscore>3) | (data_zscore.zscore<-3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2RQ911ko7rG7"
      },
      "cell_type": "code",
      "source": [
        "df_no_outliers_zscore = data_zscore[(data_zscore.zscore>-3) & (data_zscore.zscore<3)]\n",
        "df_no_outliers_zscore.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IJJcpeIb7rG7"
      },
      "cell_type": "code",
      "source": [
        "data_box_plot = pd.read_csv('../input/titanic/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mVXHhWfD7rG7"
      },
      "cell_type": "markdown",
      "source": [
        "#### Let us first look at the distribution of age feature using histogram as shown below"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6iOxGstx7rG7"
      },
      "cell_type": "code",
      "source": [
        "fig = data_box_plot.Age.hist(bins=50)\n",
        "fig.set_title('Age Distribution')\n",
        "fig.set_xlabel('Age')\n",
        "fig.set_ylabel('Number of Passengers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vi0x2__47rG8"
      },
      "cell_type": "markdown",
      "source": [
        "Now let us look at the boxplot of age feature"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ipmQYFv_7rG8"
      },
      "cell_type": "code",
      "source": [
        "fig = data_box_plot.boxplot(column='Age')\n",
        "fig.set_title('')\n",
        "fig.set_xlabel('Survived')\n",
        "fig.set_ylabel('Age')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "q7uGIAkB7rG8"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GIR6iOcz7rG8"
      },
      "cell_type": "code",
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(14,10))\n",
        "OverallQual_scatter_plot = pd.concat([train['SalePrice'],train['GarageArea']],axis = 1)\n",
        "sns.regplot(x='GarageArea',y = 'SalePrice',data = OverallQual_scatter_plot,scatter= True, fit_reg=True, ax=ax1)\n",
        "TotalBsmtSF_scatter_plot = pd.concat([train['SalePrice'],train['TotalBsmtSF']],axis = 1)\n",
        "sns.regplot(x='TotalBsmtSF',y = 'SalePrice',data = TotalBsmtSF_scatter_plot,scatter= True, fit_reg=True, ax=ax2)\n",
        "GrLivArea_scatter_plot = pd.concat([train['SalePrice'],train['GrLivArea']],axis = 1)\n",
        "sns.regplot(x='GrLivArea',y = 'SalePrice',data = GrLivArea_scatter_plot,scatter= True, fit_reg=True, ax=ax3)\n",
        "GarageArea_scatter_plot = pd.concat([train['SalePrice'],train['BsmtFinSF1']],axis = 1)\n",
        "sns.regplot(x='BsmtFinSF1',y = 'SalePrice',data = GarageArea_scatter_plot,scatter= True, fit_reg=True, ax=ax4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yO8NRTff7rG9"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# load the numerical variables of the Titanic Dataset\n",
        "data = pd.read_csv('../input/titanic/train.csv', usecols = ['Pclass', 'Age', 'Fare', 'Survived'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lsdpynDP7rG9"
      },
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bLPjo7R77rG9"
      },
      "cell_type": "code",
      "source": [
        "# let's look at missing data\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Fs3k2DLG7rG-"
      },
      "cell_type": "code",
      "source": [
        "# let's separate into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Pclass', 'Age', 'Fare']],\n",
        "                                                    data.Survived, test_size=0.3,\n",
        "                                                    random_state=0)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "M8FaOrEM7rG-"
      },
      "cell_type": "code",
      "source": [
        "# let's fill first the missing data\n",
        "\n",
        "X_train.Age.fillna(X_train.Age.median(), inplace=True)\n",
        "X_test.Age.fillna(X_train.Age.median(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_uMZFwu67rG-"
      },
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler() # create an object\n",
        "X_train_scaled = scaler.fit_transform(X_train) # fit the scaler to the train set, and then transform it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "eVPgYp1V7rG-"
      },
      "cell_type": "code",
      "source": [
        "#let's have a look at the scaled training dataset: mean and standard deviation\n",
        "print('means (Pclass, Age and Fare): ', X_train_scaled.mean(axis=0))\n",
        "print('std (Pclass, Age and Fare): ', X_train_scaled.std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "umIDA3S17rG-"
      },
      "cell_type": "code",
      "source": [
        "# let's look at the transformed min and max values\n",
        "print('Min values (Pclass, Age and Fare): ', X_train_scaled.min(axis=0))\n",
        "print('Max values (Pclass, Age and Fare): ', X_train_scaled.max(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sWun8OS-7rG-"
      },
      "cell_type": "code",
      "source": [
        "# let's look at the distribution of the transformed variable Age\n",
        "plt.hist(X_train_scaled[:,1], bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PrnFM8zD7rG_"
      },
      "cell_type": "code",
      "source": [
        "# let's look at how transformed age looks like compared to the original variable\n",
        "sns.jointplot(X_train.Age, X_train_scaled[:,1], kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IakkS46c7rHA"
      },
      "cell_type": "code",
      "source": [
        "minmaxscaler = MinMaxScaler() # create an object\n",
        "X_train_scaled = minmaxscaler.fit_transform(X_train) # fit the scaler to the train set, and then transform it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HQKkGUt-7rHB"
      },
      "cell_type": "code",
      "source": [
        "#let's have a look at the scaled training dataset: mean and standard deviation\n",
        "print('means (Pclass, Age and Fare): ', X_train_scaled.mean(axis=0))\n",
        "print('std (Pclass, Age and Fare): ', X_train_scaled.std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "apufw-R27rHB"
      },
      "cell_type": "code",
      "source": [
        "# let's look at the transformed min and max values\n",
        "print('Min values (Pclass, Age and Fare): ', X_train_scaled.min(axis=0))\n",
        "print('Max values (Pclass, Age and Fare): ', X_train_scaled.max(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Lw1VsEZg7rHC"
      },
      "cell_type": "code",
      "source": [
        "# let's look at how transformed age looks like compared to the original variable\n",
        "sns.jointplot(X_train.Age, X_train_scaled[:,1], kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JSbb7pin7rHC"
      },
      "cell_type": "code",
      "source": [
        "maxscaler = MaxAbsScaler() # create an object\n",
        "X_train_scaled = maxscaler.fit_transform(X_train) # fit the scaler to the train set, and then transform it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rKgyjbwh7rHC"
      },
      "cell_type": "code",
      "source": [
        "#let's have a look at the scaled training dataset: mean and standard deviation\n",
        "print('means (Pclass, Age and Fare): ', X_train_scaled.mean(axis=0))\n",
        "print('std (Pclass, Age and Fare): ', X_train_scaled.std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yvqOtDbb7rHD"
      },
      "cell_type": "code",
      "source": [
        "# let's look at the transformed min and max values\n",
        "print('Min values (Pclass, Age and Fare): ', X_train_scaled.min(axis=0))\n",
        "print('Max values (Pclass, Age and Fare): ', X_train_scaled.max(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3_GRF2Yw7rHD"
      },
      "cell_type": "code",
      "source": [
        "# let's look at how transformed age looks like compared to the original variable\n",
        "sns.jointplot(X_train.Age, X_train_scaled[:,1], kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WcF61nKu7rHD"
      },
      "cell_type": "code",
      "source": [
        "robustscaler = RobustScaler() # create an object\n",
        "X_train_scaled = robustscaler.fit_transform(X_train) # fit the scaler to the train set, and then transform it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lwE5TJnk7rHE"
      },
      "cell_type": "code",
      "source": [
        "#let's have a look at the scaled training dataset: mean and standard deviation\n",
        "print('means (Pclass, Age and Fare): ', X_train_scaled.mean(axis=0))\n",
        "print('std (Pclass, Age and Fare): ', X_train_scaled.std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "s8qug2Ud7rHE"
      },
      "cell_type": "code",
      "source": [
        "# let's look at the transformed min and max values\n",
        "print('Min values (Pclass, Age and Fare): ', X_train_scaled.min(axis=0))\n",
        "print('Max values (Pclass, Age and Fare): ', X_train_scaled.max(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qK-IXdQs7rHE"
      },
      "cell_type": "code",
      "source": [
        "# let's look at how transformed age looks like compared to the original variable\n",
        "sns.jointplot(X_train.Age, X_train_scaled[:,1], kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "OpjKo0CI7rHE"
      },
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer() # create an object\n",
        "X_train_scaled = normalizer.fit_transform(X_train) # fit the scaler to the train set, and then transform it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TIbgvztI7rHE"
      },
      "cell_type": "code",
      "source": [
        "#let's have a look at the scaled training dataset: mean and standard deviation\n",
        "print('means (Pclass, Age and Fare): ', X_train_scaled.mean(axis=0))\n",
        "print('std (Pclass, Age and Fare): ', X_train_scaled.std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sITqkAMY7rHE"
      },
      "cell_type": "code",
      "source": [
        "# let's look at the transformed min and max values\n",
        "print('Min values (Pclass, Age and Fare): ', X_train_scaled.min(axis=0))\n",
        "print('Max values (Pclass, Age and Fare): ', X_train_scaled.max(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1I5xeEOT7rHE"
      },
      "cell_type": "code",
      "source": [
        "# let's look at how transformed age looks like compared to the original variable\n",
        "sns.jointplot(X_train.Age, X_train_scaled[:,1], kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1XVZoF3x7rHF"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "quantileTransformer = QuantileTransformer()\n",
        "X_train_scaled = quantileTransformer.fit_transform(X_train) # fit the scaler to the train set, and then transform it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GirzPlxV7rHF"
      },
      "cell_type": "code",
      "source": [
        "#let's have a look at the scaled training dataset: mean and standard deviation\n",
        "print('means (Pclass, Age and Fare): ', X_train_scaled.mean(axis=0))\n",
        "print('std (Pclass, Age and Fare): ', X_train_scaled.std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8fQlGfVx7rHF"
      },
      "cell_type": "code",
      "source": [
        "# let's look at the transformed min and max values\n",
        "print('Min values (Pclass, Age and Fare): ', X_train_scaled.min(axis=0))\n",
        "print('Max values (Pclass, Age and Fare): ', X_train_scaled.max(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gVSa-QUi7rHF"
      },
      "cell_type": "code",
      "source": [
        "# let's look at how transformed age looks like compared to the original variable\n",
        "sns.jointplot(X_train.Age, X_train_scaled[:,1], kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EwX_g0n67rHF"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "powerTransformer = PowerTransformer()\n",
        "X_train_scaled = powerTransformer.fit_transform(X_train) # fit the scaler to the train set, and then transform it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2VIHQi1-7rHF"
      },
      "cell_type": "code",
      "source": [
        "#let's have a look at the scaled training dataset: mean and standard deviation\n",
        "print('means (Pclass, Age and Fare): ', X_train_scaled.mean(axis=0))\n",
        "print('std (Pclass, Age and Fare): ', X_train_scaled.std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4H_KA5h07rHG"
      },
      "cell_type": "code",
      "source": [
        "# let's look at the transformed min and max values\n",
        "print('Min values (Pclass, Age and Fare): ', X_train_scaled.min(axis=0))\n",
        "print('Max values (Pclass, Age and Fare): ', X_train_scaled.max(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "v7M81MeN7rHG"
      },
      "cell_type": "code",
      "source": [
        "# let's look at how transformed age looks like compared to the original variable\n",
        "sns.jointplot(X_train.Age, X_train_scaled[:,1], kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VJcjil_K7rHG"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('../input/birdsong-recognition/train.csv')\n",
        "data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pb1hOhs97rHG"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('../input/birdsong-recognition/train.csv')\n",
        "data['date'] = pd.to_datetime(data['date'],format='%Y-%m-%d', errors='coerce')\n",
        "\n",
        "data['year']=data['date'].dt.year\n",
        "data['month']=data['date'].dt.month\n",
        "data['day']=data['date'].dt.day\n",
        "data['dayofweek_num']=data['date'].dt.dayofweek\n",
        "data['dayofweek_name']=data['date'].dt.weekday\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "BPlcG6MS7rHH"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('../input/birdsong-recognition/train.csv')\n",
        "data['time'] = pd.to_datetime(data['date'],format='%Y-%m-%d', errors='coerce')\n",
        "\n",
        "data['time'] = pd.to_datetime(data['time'],format='%H:%M')\n",
        "\n",
        "data['Hour'] = data['time'].dt.hour\n",
        "data['minute'] = data['time'].dt.minute\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cQOy8HuM7rHH"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
        "data['YrSold'] = pd.to_datetime(data['YrSold'],format='%Y')\n",
        "\n",
        "data['lag_1'] = data['SalePrice'].shift(1)\n",
        "data = data[['YrSold', 'lag_1', 'SalePrice']]\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bJASC1bw7rHH"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
        "data['YrSold'] = pd.to_datetime(data['YrSold'],format='%Y')\n",
        "\n",
        "data['lag_1'] = data['SalePrice'].shift(1)\n",
        "data['lag_2'] = data['SalePrice'].shift(2)\n",
        "data['lag_3'] = data['SalePrice'].shift(3)\n",
        "data['lag_4'] = data['SalePrice'].shift(4)\n",
        "data['lag_5'] = data['SalePrice'].shift(5)\n",
        "data['lag_6'] = data['SalePrice'].shift(6)\n",
        "data['lag_7'] = data['SalePrice'].shift(7)\n",
        "data = data[['YrSold', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'SalePrice']]\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DcOrVqAJ7rHI"
      },
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
        "plot_acf(data['SalePrice'], lags=10)\n",
        "plot_pacf(data['SalePrice'], lags=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WCru2FcU7rHI"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
        "data['YrSold'] = pd.to_datetime(data['YrSold'],format='%Y')\n",
        "\n",
        "data['rolling_mean'] = data['SalePrice'].rolling(window=7).mean()\n",
        "data = data[['YrSold', 'rolling_mean', 'SalePrice']]\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HOyznlWw7rHI"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
        "data['YrSold'] = pd.to_datetime(data['YrSold'],format='%Y')\n",
        "\n",
        "data['Expanding_Mean'] = data['SalePrice'].expanding(2).mean()\n",
        "data = data[['YrSold', 'SalePrice','Expanding_Mean']]\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}